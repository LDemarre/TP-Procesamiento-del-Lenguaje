{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fb5afa",
   "metadata": {},
   "source": [
    "# **Trabajo Práctico - Procesamiento del Lenguaje Natural**\n",
    "# *Integrante: Lucas Demarré*\n",
    "# *Año: 2024*\n",
    "\n",
    "---\n",
    "\n",
    "# **Tabla de contenidos**\n",
    "\n",
    "1.   [**Introducción**](#)\n",
    "2.   [**Librerías a Utilizar**](#)\n",
    "3.   [**Variables Globales**](#)\n",
    "4.   [**Definición de Funciones**](#)\n",
    "        1.   [*Funciones para Creación de Fuentes de Información*](#)\n",
    "        2.   [*Funciones para Manejo de Datos PDF*](#)\n",
    "        3.   [*Funciones para Manejo de Datos CSV*](#)\n",
    "        4.   [*Funciones para la creación y consulta de Base de Datos de Grafos*](#)\n",
    "        5.   [*Funciones para Clasificación de Intenciones*](#)\n",
    "        6.   [*Funciones para Generar Respuesta del Chatbot*](#)\n",
    "5.   [**Creación de Fuentes de Información**](#)\n",
    "6.   [**Carga y Limpieza de Datos**](#)\n",
    "        1.   [*Documentos PDF*](#)\n",
    "        2.   [*Datos Tabulares (CSV)*](#)\n",
    "        3.   [*Recolección y Etiquetado de Preguntas para Clasificación de Intenciones*](#)\n",
    "7.   [**Procesamiento de Datos para Base de Datos Chroma (PDFs)**](#)\n",
    "        1.   [*División de los Documentos en Pequeñas Partes*](#)\n",
    "        2.   [*Conversión a Embeddings y Almacenamiento en Chroma*](#)\n",
    "8.   [**Creación de la Base de Datos de Grafos**](#)\n",
    "9.   [**Entrenamiento del Clasificador de Intenciones**](#)\n",
    "        1.   [*Cargamos el Modelo de Embeddings*](#)\n",
    "        2.   [*División en Conjuntos de Entrenamiento y Prueba y Tokenización*](#)\n",
    "        3.   [*Entrenamiento y Evaluación del Modelo Clasificador*](#)\n",
    "        4.   [*Validación del Modelo Clasificador*](#)\n",
    "10.   [**Integración y Flujo de Trabajo del Chatbot**](#)\n",
    "        1.   [*Definimos la consulta a hacer*](#)\n",
    "        2.   [*Clasificación de Preguntas de Usuario*](#)\n",
    "        3.   [*Búsqueda en la Base de Datos Correspondiente y Generación del Prompt*](#)\n",
    "        4.   [*Generación de Respuesta con el Chatbot*](#)\n",
    "11.  [**Demo del Chatbot**](#)\n",
    "12.  [**Informe sobre Agentes Inteligentes (LLM libres)**](#)\n",
    "       1.    [*Estado del Arte de las Aplicaciones Actuales*](#)\n",
    "       2.    [*Problemática a Solucionar con un Sistema Multiagente*](#)\n",
    "       3.    [*Ejemplo de conversación*](#)\n",
    "       4.    [*Fuentes de Información*](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24144c9d-519b-4397-8eac-05564b69a237",
   "metadata": {},
   "source": [
    "# 1. **Introducción**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683f088-87ff-4ffb-a69d-21a726493e0a",
   "metadata": {},
   "source": [
    "Este proyecto trata de el diseño y creación de un **ChatBot Experto** especializado en **Videojuegos**. Este ChatBot se construyó con la capacidad de responder consultas específicas sobre el tema, utilizando un vasto conocimiento extraído de diversos títulos. La información que abarca incluye *géneros*, *descripciones*, *desarrolladores*, *distribuidoras*, *requisitos de sistema* y *precios*, almacenada inicialmente en archivos PDF's y posteriormente transferida a bases de datos para su fácil acceso y consulta.\n",
    "\n",
    "### ¿Qué es el **Chatbot**?\n",
    "El **Chatbot** representa el conjunto de todos los pasos a detallar, empezando en la creación de base de datos, utilización de embeddings o no dependiendo de la base de datos creada y el modelo de Lenguaje de Gran Tamaño utilizado para la posterior consulta.\n",
    "\n",
    "## Elección de tema:\n",
    "El primer paso en el desarrollo de este ChatBot es seleccionte el tema de interés, que para este proyecto son los videojuegos. La información se extrae de múltiples fuentes y se organiza en un formato estructurado para su posterior uso.\n",
    "\n",
    "## Creación de Bases de Datos:.\n",
    "La creación de bases de datos adecuadas es esencial, ya que actúan como el fundamento sobre el cual el ChatBot basa sus respuestas. Este proceso implica la organización de la información en tres tipos principales de bases de datos:\n",
    "\n",
    "1. **Base de Datos Vectorial:** Aquí se almacenan oraciones transformadas en vectores a través de un proce*so de emb*edding, permitiendo su representación en un espacio multidimensional. Esto facilita la identificación de similitudes y diferencias entre consultas y la información almacenada, optimizando la precisión en las respuestas del ChatBot.\n",
    "   \n",
    "2. **Base de Datos de Grafos:** Utiliza una estructura de grafos para almacenar información, donde los nodos representan datos o conceptos interconectados por aristas. Esta estructura permite una búsqueda eficiente y flexible a través de las conexiones, facilitando el acceso a información relevante basada en la relación entre diferentes nodos.\n",
    "   \n",
    "3. **Datos Tabulares:** Estos se presentan en un formato más sencillo, almacenados en archivos CSV que se pueden consultar de manera directa, similar a un diccionario, accediendo a la información mediante claves y columnas.\n",
    "\n",
    "## Proceso Final que lleva a cabo el **Chatbot**:\n",
    "Para interactuar efectivamente con el ChatBot y extraer la información necesaria de estas bases de datos, se aplican diversas técnicas de Procesamiento de Lenguaje Natural (NLP). Esto incluye desde la extracción de palabras clave hasta la conversión de consultas completas en vectores, dependiendo de la naturaleza de la base de datos consultada. Además, se implementan procesos de limpieza de datos para optimizar el contexto proporcionado al modelo de Lenguaje de Gran Tamaño (LLM), que es el encargado de interpretar las consultas y generar respuestas coherentes y precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb694ae9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. **Librerías a utilizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53706074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import chromadb\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import networkx as nx\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from fuzzywuzzy import process\n",
    "from reportlab.lib.units import inch\n",
    "from langchain.schema import Document\n",
    "from IPython.display import clear_output\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from pdfminer.high_level import extract_text\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db804ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. **Variables Globales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60a5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos nuestra KEY para acceder a la API de HuggingFace\n",
    "API_KEY = 'TU-API-KEY'\n",
    "\n",
    "# Establecemos rutas de acceso para acceder/guardar los archivos necesarios\n",
    "CHROMA_PATH = 'chroma' # donde se va a guardar la base de datos 'chroma'\n",
    "PDF_PATH = 'databases/vectorDB' # ruta de los archivos PDF\n",
    "CSV_PATH = 'databases/tabularDB/Games Price.csv' # ruta del archivo CSV\n",
    "\n",
    "# Creamos una lista de etiquetas llamada que contiene pares de valores para categorizar las consultas al LLM\n",
    "labels = [(0, 'Consulta PDF'), (1, 'Consulta CSV'), (2, 'Consulta Grafos')]\n",
    "\n",
    "# Establecemos el nombre del modelo de 'Embedding' que utilizaremos para convertir el texto en vectores\n",
    "model_embedding_chatbot = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n",
    "\n",
    "# Definimos la plantilla de texto que usaremos como 'prompt' para generar una respuesta del LLM más estructurada\n",
    "PROMPT_TEMPLATE = '''\n",
    "La información de contexto es la siguiente:\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Dada la información de contexto anterior, y sin utilizar conocimiento previo, responde la siguiente pregunta.\n",
    "Pregunta: {question}\n",
    "Respuesta: '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df211",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. **Definición de Funciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199fc41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.1. *Funciones para la Creación de Fuentes de Información*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1caaf-c3c1-4c7c-a315-c6ddc4390156",
   "metadata": {},
   "source": [
    "En el proyecto opté por generar información propia en lugar de depender de fuentes externas. Esta decisión se basa en experiencias previas con la API de **Steam**, una plataforma líder a nivel mundial en la distribución de videojuegos. A pesar de que **Steam** ofrece acceso público a una vasta cantidad de datos a través de su API, la elección de construir *fuentes de conocimientos personalizadas* surge de otras cuestiones:\n",
    "\n",
    "- **Formato Personalizado:** Crear las propias fuentes de información permite adaptar el formato de los datos a las necesidades específicas del proyecto, facilitando su integración y manipulación en el diseño del **ChatBot**.\n",
    "- **Experiencia Previa:** La familiaridad previa con la API de **Steam** ofrece una ventaja técnica, permitiendo una extracción de datos más eficiente y efectiva.\n",
    "\n",
    "Sin embargo, la tarea de recopilar datos de **Steam** presenta sus propios desafíos, principalmente debido a la vastedad del catálogo disponible y las limitaciones de la API. **Steam** cuenta con más de 180.000 títulos, lo que hace inviable la extracción completa de datos en un marco de tiempo razonable. Por lo tanto, se estableció un límite práctico para extraer información de los primeros 10.000 títulos, lo cual ya de por sí tarda más de 6 horas. Aun así, a pesar de la limitación, se cumplen con creces los requisitos establecidos en el trabajo, tan solo el PDF de *Requirements* tiene 171 páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e036f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Definimos una función para obtener los nombres de los juegos desde una API '''\n",
    "def bring_game_names():\n",
    "    api_url = 'https://api.steampowered.com/ISteamApps/GetAppList/v2/' # Especificamos la URL de la API de Steam\n",
    "    response = rq.get(api_url) # Realizamos la solicitud a la API\n",
    "    \n",
    "    appids = [] # Inicializamos una lista para almacenar los IDs y nombres de los juegos\n",
    "    \n",
    "    # Verificamos si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        data = response.json() # Convertimos la respuesta en formato JSON\n",
    "        \n",
    "        # Recorremos la lista de juegos y añadimos sus IDs y nombres a nuestra lista\n",
    "        for i in data['applist']['apps']: appids.append([str(i['appid']), str(i['name'])])\n",
    "    return appids\n",
    "\n",
    "''' Definimos una función para limpiar texto que contiene etiquetas HTML '''\n",
    "def clean_text(text):\n",
    "    # Usamos BeautifulSoup para parsear el HTML\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Obtenemos solo el texto, sin etiquetas HTML\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "''' Definimos una función para detectar el idioma de un texto '''\n",
    "def detect_language(text):\n",
    "    try: return detect(text) # Intentamos detectar el idioma y lo devolvemos\n",
    "    except: return \"Error en la detección\" # En caso de error, devolvemos un mensaje de error\n",
    "\n",
    "''' Definimos una función con cuatro parámetros para detectar si un valor se encuentra o no en el json '''\n",
    "def get_value(data, key, default, transform=None):\n",
    "    '''\n",
    "    `data`: el diccionario de donde queremos obtener el valor.\n",
    "    `key`: la clave que estamos buscando en el diccionario.\n",
    "    `default`: el valor por defecto que se devuelve si la clave no se encuentra.\n",
    "    `transform`: una función opcional para transformar el valor obtenido; por defecto es None. \n",
    "    '''\n",
    "    \n",
    "    # Se intenta obtener el valor asociado con 'key' en el diccionario 'data'.\n",
    "    # Si 'key' no se encuentra en 'data', se devuelve 'default'.\n",
    "    value = data.get(key, default)\n",
    "\n",
    "    # Si 'transform' no es None, se aplica 'transform' a 'value'.\n",
    "    # Si 'transform' es None, simplemente se devuelve 'value'.\n",
    "    return transform(value) if transform else value\n",
    "    \n",
    "''' Definimos una función para obtener detalles de los juegos '''\n",
    "def game_details(games):\n",
    "    start = time.time() # Marcamos el tiempo de inicio\n",
    "    \n",
    "    # Especificamos la URL de la API para detalles de juegos individuales\n",
    "    api_url_item = 'https://store.steampowered.com/api/appdetails?appids='\n",
    "    games_seen = 0\n",
    "    total_games = len(games)\n",
    "    \n",
    "    # Creamos documentos PDF para diferentes categorías de información\n",
    "    game_descriptions = SimpleDocTemplate('databases/vectorDB/Games Description.pdf', pagesize=letter)\n",
    "    game_requirements = SimpleDocTemplate('databases/vectorDB/Games Requirement.pdf', pagesize=letter)\n",
    "    game_details = SimpleDocTemplate('databases/graphDB/Games Detail.pdf', pagesize=letter)\n",
    "    \n",
    "    # Inicializamos listas para almacenar contenido de los PDFs\n",
    "    story_gd = []\n",
    "    story_gr = []\n",
    "    story_gde = []\n",
    "    \n",
    "    styles = getSampleStyleSheet() # Obtenemos un conjunto estándar de estilos para los PDFs\n",
    "    output_filename = 'databases/tabularDB/Games Price.csv' # Especificamos el nombre del archivo CSV a crear\n",
    "    \n",
    "    # Abrimos el archivo CSV para escritura\n",
    "    with open(output_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['nombre', 'precio']) # Escribimos la fila de encabezado en el CSV\n",
    "\n",
    "        # Iteramos sobre la lista de juegos mientras haya juegos disponibles\n",
    "        while games:        \n",
    "            games_seen += 1\n",
    "            \n",
    "            # Extramos el último elemento de la lista y guardamos el ID y el nombre del juego\n",
    "            game = games.pop()\n",
    "            game_id = game[0]\n",
    "            game_name = game[1]\n",
    "            \n",
    "            # Realizamos una solicitud a la API para obtener detalles del juego\n",
    "            response = rq.get(api_url_item + str(game_id) + '&l=spanish')\n",
    "            \n",
    "            try:\n",
    "                if response.status_code == 200: \n",
    "                    temp = response.json()\n",
    "\n",
    "                    if temp[str(game_id)]['success']:\n",
    "\n",
    "                        # Verificamos que el juego tiene categorías\n",
    "                        if temp.get(str(game_id), {}).get('data', {}).get('categories'):\n",
    "                            # Guardamos las categorías en una lista\n",
    "                            data = temp[str(game_id)]['data'] \n",
    "                            game_caterogies = data['categories']\n",
    "                            categories_list = [category['id'] for category in game_caterogies]\n",
    "\n",
    "                            # Verificamos que no sea una demo o una banda sonora\n",
    "                            if 21 not in categories_list and 'Demo' not in game_name and 'Soundtrack' not in game_name:\n",
    "                                recommended_req = 'No especificado'\n",
    "                                minimun_req = 'No especificado'\n",
    "\n",
    "                                if type(data.get('pc_requirements', {})) != list:\n",
    "                                    recommended_req = get_value(data['pc_requirements'], 'recommended', 'No especificado', clean_text)\n",
    "                                    minimun_req = get_value(data['pc_requirements'], 'minimum', 'No especificado', clean_text)\n",
    "\n",
    "                                price = get_value(data, 'price_overview', 'Gratis', lambda x: x['final_formatted'] if isinstance(x, dict) else x)\n",
    "                                developers = get_value(data, 'developers', 'No especificado', lambda x: x[0])\n",
    "                                publishers = get_value(data, 'publishers', 'No especificado', lambda x: x[0])\n",
    "                                short_description = get_value(data, 'short_description', 'No especificado', clean_text)\n",
    "\n",
    "                                # Detectamos el idioma de las descripciones\n",
    "                                language_short = detect_language(short_description)\n",
    "\n",
    "                                # Si el idioma es español, añadimos la información a los documentos PDF y al CSV\n",
    "                                if language_short == 'es':    \n",
    "                                    story_gd.append(Paragraph(f'<b>Descripción corta de {game_name}</b>: {short_description}', styles['Normal']))\n",
    "                                    story_gd.append(Paragraph('<br/><br/>', styles['Normal']))\n",
    "\n",
    "                                    story_gr.append(Paragraph(f'<b>Requisitos mínimos de {game_name}</b>: {minimun_req}', styles['Normal']))\n",
    "                                    story_gr.append(Paragraph('<br/><br/>', styles['Normal']))  \n",
    "                                    story_gr.append(Paragraph(f'<b>Requisitos recomendados de {game_name}</b>: {recommended_req}', styles['Normal']))\n",
    "                                    story_gr.append(Paragraph('<br/><br/>', styles['Normal']))  \n",
    "\n",
    "                                    story_gde.append(Paragraph(f'<b>Generos de {game_name}</b>:', styles['Normal']))              \n",
    "                                    if 'genres' in data:\n",
    "                                        for genre in data['genres']: story_gde.append(Paragraph(genre['description'], styles['Normal']))\n",
    "                                    else: story_gde.append(Paragraph('No especificado', styles['Normal']))\n",
    "\n",
    "                                    story_gde.append(Paragraph(f'<b>Desarrollador de {game_name}</b>: {developers}', styles['Normal']))\n",
    "                                    story_gde.append(Paragraph('<br/>', styles['Normal']))  \n",
    "                                    story_gde.append(Paragraph(f'<b>Distribuidor de {game_name}</b>: {publishers}', styles['Normal']))\n",
    "                                    story_gde.append(Paragraph('<br/><br/>', styles['Normal']))  \n",
    "\n",
    "                                    writer.writerow([game_name, price])\n",
    "\n",
    "                    print(f'Progreso: {((games_seen / total_games) * 100):.3f}%.     ', end='\\r')\n",
    "\n",
    "                # Si la respuesta del request nos devuelve el codigo 429 significa que llegamos el limite de la API\n",
    "                # Solo se pueden hacer 200 llamadas cada 5 minutos, por lo que se espera 5 minutos y se sigue\n",
    "                elif response.status_code == 429: \n",
    "                    print(f'Esperando 5 minutos...', end='\\r')\n",
    "\n",
    "                    games_seen -= 1\n",
    "                    games.append(game) # Devolvemos el juego ya que este no se analizo\n",
    "\n",
    "                    time.sleep(300)\n",
    "                else: print(f'Progreso: {((games_seen / total_games) * 100):.3f}%.', end='\\r')\n",
    "            except JSONDecodeError: pass\n",
    "            except requests.exceptions.RequestException as e: pass\n",
    "            finally: pass\n",
    "    \n",
    "    # Construimos los documentos PDF con la información recopilada\n",
    "    game_descriptions.build(story_gd)\n",
    "    game_requirements.build(story_gr)\n",
    "    game_details.build(story_gde)               \n",
    "    \n",
    "    # Calculamos el tiempo total transcurrido\n",
    "    end = time.time()\n",
    "    duracion_segundos = end - start\n",
    "    horas = duracion_segundos // 3600\n",
    "    minutos = (duracion_segundos % 3600) // 60\n",
    "    segundos = duracion_segundos % 60\n",
    "    \n",
    "    print(f'PDFs y CSV creados al completo. Tiempo transcurrido: {int(horas)}:{int(minutos)}:{int(segundos)}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7f619",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.2. *Funciones para Manejo de Datos PDF*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2762f-2d39-403a-bab3-cfb17c404c07",
   "metadata": {},
   "source": [
    "La etapa de limpieza y preparación del texto extraído de los PDFs es fundamental para asegurar que la información esté en un formato adecuado y listo para ser utilizado por el ChatBot. Este proceso se realiza en dos fases principales: limpieza y división del texto.\n",
    "\n",
    "### Limpieza del texto:\n",
    "La limpieza del texto implica varios pasos diseñados para eliminar caracteres no deseados y preparar el texto para su posterior procesamiento:\n",
    "\n",
    "1. **Eliminación de Caracteres Específicos**: Se remueven caracteres que no aportan valor y que resultan de la extracción de los PDFs, como *\\x0c* y *[/]*. Estos caracteres suelen ser artefactos de la conversión de formato y no forman parte del contenido útil.\n",
    "\n",
    "2. **Manejo de Saltos de Línea**: Los dobles saltos de línea dentro de los documentos indican la separación entre secciones y son clave para la estructura del documento. Para preservar esta estructuración, se reemplazan temporalmente con un marcador específico. Esto permite diferenciarlos de los saltos de línea individuales, que sí se desean eliminar o reemplazar, manteniendo así la distinción entre los espacios dentro de un sección y los que delimitan distintos secciones. Luego de ello se vuelve a convertir el marcador en los dos saltos de línea consecutivos.\n",
    "\n",
    "3. **Limpieza de Espacios en Blanco**: Se eliminan espacios en blanco innecesarios, como múltiples espacios seguidos o espacios al inicio y final de los párrafos, para homogeneizar el texto y facilitar su procesamiento.\n",
    "\n",
    "### División del Texto:\n",
    "Una vez el texto ha sido limpiado, el siguiente paso es dividirlo en partes más manejables, basándose en la estructura identificada durante la fase de limpieza:\n",
    "\n",
    "1. **Uso de Marcadores de División**: Ahora se puede utilizar los dos saltos de líneas consecutivos para dividir el texto en secciones de manera eficiente. Este método asegura que la división se base en la estructura original del documento, respetando la separación intencionada entre secciones.\n",
    "\n",
    "2. **Identificación de Textos de Inicio de Párrafo**: Gracias a la preparación y formato personalizado de las fuentes de conocimiento, es posible identificar patrones o textos que denoten el comienzo de nuevos párrafos o secciones. Utilizando estos indicadores, junto con los marcadores de división, se facilita aún más la segmentación precisa del texto.\n",
    "\n",
    "Este proceso de limpieza y división es esencial para transformar el contenido extraído de los PDFs en un recurso listo para ser consultado y analizado por el ChatBot. Al garantizar que el texto esté libre de elementos superfluos y estructurado de manera coherente, se facilita la extracción de información relevante y la generación de respuestas precisas a las consultas de los usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e84b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Definimos una función para cargar los documentos PDF desde la ruta especificada '''\n",
    "def load_pdf_documents(pdf_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(pdf_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_path, filename)\n",
    "            text = extract_text(file_path)\n",
    "            documents.append(text)\n",
    "    return documents\n",
    "\n",
    "''' Definimos una función para limpiar los documentos del texto extraído de los PDFs '''\n",
    "def clean_documents(documents):\n",
    "    cleaned_documents = [] # Creamos una lista vacía para almacenar los documentos limpios\n",
    "    \n",
    "    # Iteramos sobre cada documento\n",
    "    for doc in documents:\n",
    "        # Eliminamos el símbolo '\\x0c'\n",
    "        clean_text = doc.replace('\\x0c', '')\n",
    "        \n",
    "        # Sustituimos el símbolo [/] por espacio\n",
    "        clean_text = clean_text.replace('[/]', ' ')\n",
    "        \n",
    "        # Conservamos los dobles saltos de línea reemplazándolos temporalmente por un marcador\n",
    "        clean_text = re.sub(r'(\\n\\n)', 'DOUBLENEWLINE', clean_text)\n",
    "        \n",
    "        # Eliminamos los saltos de línea individuales\n",
    "        clean_text = re.sub(r'\\n', ' ', clean_text)\n",
    "        \n",
    "        # Revertimos los dobles saltos de línea a su forma original\n",
    "        clean_text = clean_text.replace('DOUBLENEWLINE', '\\n\\n')\n",
    "        \n",
    "        # Eliminamos espacios múltiples\n",
    "        clean_text = re.sub(r' +', ' ', clean_text)\n",
    "        \n",
    "        # Lo añadimos a la lista de documentos limpios\n",
    "        cleaned_documents.append(clean_text)\n",
    "    return cleaned_documents\n",
    "\n",
    "''' Definimos una función para dividir el texto de los documentos en segmentos más pequeños '''\n",
    "def split_text(documents):\n",
    "    '''\n",
    "    Condiciones para hacer split:\n",
    "        1. Que haya dos saltos de líneas iniciales, estos representan un párrafo.\n",
    "        2. Que después de esos dos saltos de líneas le siga alguna de las siguientes combinaciones de palabras:\n",
    "                A. 'Descripción corta de'\n",
    "                B. 'Requisitos mínimos'\n",
    "                C. 'Requisitos recomendados'\n",
    "           \n",
    "           Esto es para que no se haga split en algún lugar donde haya dos saltos de líneas cumpliendo la primera condición\n",
    "           pero esos saltos uno es porque el texto sigue en una página nueva, por lo que estaríamos cortando texto nuevo.\n",
    "    '''\n",
    "    \n",
    "    # Patrones para hacer split según las condiciones mencionadas\n",
    "    patrones = r'\\n\\n(?=Descripción corta de|Requisitos mínimos|Requisitos recomendados)'\n",
    "    \n",
    "    # Dividimos el texto usando los patrones definidos\n",
    "    texts = [text for doc in documents for text in re.split(patrones, doc.strip()) if text]\n",
    "\n",
    "    # Creamos objetos 'Document' para cada segmento de texto\n",
    "    chunks = [Document(page_content=text) for text in texts]\n",
    "    \n",
    "    print(f'Se dividió {len(documents)} documentos en {len(chunks)} chunks.')\n",
    "    return chunks\n",
    "   \n",
    "''' Definimos una función para guardar los segmentos de texto en una base de datos CHROMA. '''\n",
    "def save_to_chroma(chunks, model):\n",
    "    # Cargamos Universal Sentence Encoder\n",
    "    embed = hub.load(model)\n",
    "    \n",
    "    # Configuración inicial de ChromaDB\n",
    "    client = chromadb.Client()\n",
    "    \n",
    "    # Nombre de la colección\n",
    "    collection_name = 'my-document-collection'\n",
    "    \n",
    "    # Eliminamos la colección si ya existe\n",
    "    for col in client.list_collections():\n",
    "        if collection_name in col.name: client.delete_collection(collection_name)\n",
    "    \n",
    "    # Creamos una nueva colección\n",
    "    collection = client.create_collection(collection_name)\n",
    "    \n",
    "    # Guardamos los 'chunks'\n",
    "    textos = [doc.page_content for doc in chunks]\n",
    "    ids_textos = [f\"doc{i}\" for i in range(1, len(textos) + 1)]\n",
    "    \n",
    "    # Calculamos los embeddings para los documentos\n",
    "    embeddings = embed(textos).numpy().tolist()  # Convertimos a lista para que sea serializable\n",
    "    \n",
    "    # Almacenamos los documentos en ChromaDB\n",
    "    collection.add(\n",
    "        documents=textos,\n",
    "        ids=ids_textos,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    print(f'Se guardaron {len(chunks)} chunks.')\n",
    "    return embed, collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc363e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.3. *Funciones para Manejo de Datos CSV*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8c104-acce-4e4e-8549-0e26fb96eb2c",
   "metadata": {},
   "source": [
    "En el manejo de Datos Tabulares, se procede a eliminar los signos de dólar estadounidense y sus símbolos asociados para conservar únicamente los valores numéricos. Además, se sustituyen los títulos cuyo etiqueta es *Gratis* por el número 0, con el fin de poder manipular los datos de forma más precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec99ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Definimos una función para convertir los precios de formato texto a formato numérico '''\n",
    "def convert_price(price):\n",
    "    if price == 'Gratis' or price == 'Gratuito': return 0.0 # Si el precio es 'Gratis', lo convertimos a 0.0\n",
    "    # Sino, eliminamos el símbolo del dólar y la especificación de la moneda y convertimos el resultado a un número flotante\n",
    "    else: return float(price.replace('$', '').replace(' USD', '')) \n",
    "\n",
    "''' Definimos una función para cargar y limpiar los datos de un archivo CSV '''\n",
    "def load_csv():\n",
    "    data = pd.read_csv(CSV_PATH) # Cargamos los datos desde la ruta al archivo CSV especificado\n",
    "    data['precio'] = data['precio'].apply(convert_price) # Aplicamos la función 'convert_price' a la columna 'precio'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115568ab-ce2a-497a-8c66-39c17e07e4df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.4. *Funciones para la creación y consulta de Base de Datos de Grafos*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d900f-d956-4875-8b2c-2043e7d28072",
   "metadata": {},
   "source": [
    "En esta sección, procedemos a construir la **Base de Datos de Grafos** utilizando el texto obtenido del PDF que incluye datos relevantes sobre los *géneros*, *desarrolladores* y *distribuidores* de diversos títulos. Opté por crear personalmente la **Base de Datos de Grafos** en lugar de emplear una disponible en internet debido a la similitud en el método de acceso a la información a través de un nodo, comparado con el manejo que realicé con los datos tabulares. Este proceso implica extraer palabras clave de las consultas y utilizar la **Base de datos de Grafos** como de un diccionario se tratase, lo cual resultó ser más sencillo que intentar generar este consultas en SPARQL a partir de la consulta hecha por el usuario, tarea en la que no tuve éxito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ed5212-52b2-48e9-afb5-7ccdc9f63d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Definimos una función para extraer los datos de los PDFs '''\n",
    "def extract_data(pdf_path):\n",
    "    data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "\n",
    "            if text:\n",
    "                # Encontramos los juegos y la información correspodiente \n",
    "                info = re.findall(r'Generos de (.*?):(.*?)Desarrollador de (.*?):(.*?)Distribuidor de (.*?):(.*?)(?=\\nGeneros de|\\Z)', text, re.DOTALL)\n",
    "\n",
    "                for game in info:\n",
    "                    name = game[0].strip()\n",
    "                    gender = game[1].strip().split(',')\n",
    "                    developer = game[3].strip()\n",
    "                    publisher = game[5].strip()\n",
    "\n",
    "                    data.append({\n",
    "                        'name': name,\n",
    "                        'gender': gender,\n",
    "                        'developer': developer,\n",
    "                        'publisher': publisher\n",
    "                    })\n",
    "    return data\n",
    "\n",
    "''' Definimos una función para crear la Base de Datos de Grafos con los datos extraidos '''\n",
    "def create_graph_db():\n",
    "    data = extract_data(\"databases/graphDB/Games Detail.pdf\") # Extraemos los datos\n",
    "    \n",
    "    # Creamos el grafo\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Agregamos nodos y aristas, asegurando que los identificadores no estén vacíos\n",
    "    for game in data:\n",
    "        if game['name']:  # Aseguramos que el nombre del juego no esté vacío\n",
    "            G.add_node(game['name'], type='game')\n",
    "        \n",
    "        if game['developer']:  # Aseguramos que el nombre del desarrollador no esté vacío\n",
    "            if game['developer'] == game['publisher'] and game['developer']:\n",
    "                G.add_node(game['developer'], type='developer/publisher')\n",
    "                G.add_edge(game['name'], game['developer'])\n",
    "            else:\n",
    "                if game['developer']:  # Aseguramos que el nombre del desarrollador no esté vacío\n",
    "                    G.add_node(game['developer'], type='developer')\n",
    "                    G.add_edge(game['name'], game['developer'])\n",
    "                if game['publisher']:  # Aseguramos que el nombre del distribuidor no esté vacío\n",
    "                    G.add_node(game['publisher'], type='publisher')\n",
    "                    G.add_edge(game['name'], game['publisher'])\n",
    "    \n",
    "        for gender in game['gender']:\n",
    "            if gender:  # Aseguramos que el género no esté vacío\n",
    "                G.add_node(gender, type='gender')\n",
    "                G.add_edge(game['name'], gender)\n",
    "    return G\n",
    "\n",
    "''' Definimos funciones para consultar la Base de Datos de Grafos '''\n",
    "def query_developer(G, extracted_name):\n",
    "    return next((n for n in G.neighbors(extracted_name) if G.nodes[n]['type'] == 'developer/publisher' or G.nodes[n]['tipo'] == 'developer'), None)\n",
    "\n",
    "def query_gender(G, extracted_name):\n",
    "    return [n for n in G.neighbors(extracted_name) if G.nodes[n]['type'] == 'gender']\n",
    "\n",
    "def query_publisher(G, extracted_name):\n",
    "    return next((n for n in G.neighbors(extracted_name) if G.nodes[n]['type'] == 'developer/publisher' or G.nodes[n]['tipo'] == 'publisher'), None)\n",
    "\n",
    "''' Definimos una función para extraer el nombre del juego que se consulta '''\n",
    "def extract_name(search_string, games, similarity_threshold=80):\n",
    "    # Utilizamos fuzzywuzzy para encontrar la coincidencia más cercana\n",
    "    match, similarity = process.extractOne(search_string, games)\n",
    "    \n",
    "    # Verificamos si la similitud supera el umbral\n",
    "    if similarity >= similarity_threshold: return match\n",
    "    else: return None\n",
    "\n",
    "''' Definimos una función para extraer el tipo de consulta que se esta haciendo '''\n",
    "def extract_query_type(query):\n",
    "    # Palabras clave y algunas variaciones\n",
    "    keywords = {\n",
    "        'developer': ['desarrollador', 'desarrolladores', 'creador', 'creadores', 'hizo', 'realizado por'],\n",
    "        'gender': ['género', 'géneros', 'tipo', 'estilo'],\n",
    "        'publisher': ['distribuidor', 'distribuidores', 'publicador', 'publicadores', 'lanzado por', 'publicó']\n",
    "    }\n",
    "\n",
    "    # Aplananamos la lista de variaciones\n",
    "    variations = [(key, variation) for key, sublist in keywords.items() for variation in sublist]\n",
    "    \n",
    "    # Buscamos la coincidencia más cercana\n",
    "    best_match = process.extractOne(query, [variation for _, variation in variations], score_cutoff=60)\n",
    "    \n",
    "    # Verificamos en caso de no encontrar una coincidencia adecuada\n",
    "    if best_match:\n",
    "        match, similarity = best_match\n",
    "        query_type = next((key for key, variation in variations if variation == match), 'No se encontró coincidencia')\n",
    "    else: query_type = 'No se encontró coincidencia'\n",
    "    return query_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb4691",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.5. *Funciones para Clasificación de Intenciones*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d74262-9f8e-4163-9bd2-d73b250279a4",
   "metadata": {},
   "source": [
    "En esta fase del proyecto, se emplearán los primeros 900 títulos recopilados para entrenar un LLM con el objetivo de predecir a qué base de datos dirigirse basándose en las consultas realizadas por los usuarios. El modelo seleccionado para actuar como clasificador es **BETO: Spanish BERT**, siguiendo la metodología sugerida en la teoría.\n",
    "\n",
    "Posteriormente, las preguntas se dividen en un conjunto de entrenamiento e pruebas. Se transforman en vectores mediante técnicas de *embeddings*, preparando así el terreno para el entrenamiento del modelo. Este paso permite al modelo clasificar y predecir de manera efecti pero no necesariamente 100% exactava a qué base de datos consultar según la solicitud específica del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c3d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Definimos una función para etiquetar y crear preguntas parecidas a las que puede hacer el usuario '''\n",
    "def collection_labeling(data):\n",
    "    game_names = data['nombre'].unique() # Obtenemos los nombres únicos de los juegos del CSV  \n",
    "    dataset = [] # Inicializamos una lista vacía para almacenar las preguntas y etiquetas\n",
    "    \n",
    "    # Iteramos sobre los nombres de los juegos, limitando a los primeros 900 juegos para poder usar el resto como validación\n",
    "    for game in game_names[:900]: \n",
    "        # Añadimos preguntas etiquetadas con '0'.\n",
    "        dataset.append((0, f'¿Cuál sería la descripción corta de {game}?'))\n",
    "        dataset.append((0, f'¿De qué trata {game}?'))\n",
    "        dataset.append((0, f'¿Qué se puede hacer en {game}?'))\n",
    "\n",
    "        dataset.append((0, f'¿Cuáles son los requisitos mínimos de {game}?'))\n",
    "        dataset.append((0, f'¿Cuáles son los requisitos máximos de {game}?'))\n",
    "        dataset.append((0, f'¿Qué requisitos necesito para jugar {game}?'))\n",
    "        dataset.append((0, f'¿Qué requisitos necesito cumplir para jugar {game}?'))\n",
    "        dataset.append((0, f'¿Qué requisitos necesita mi pc para poder jugar {game}?'))\n",
    "\n",
    "        # Añadimos preguntas relacionadas etiquetadas con '1'.\n",
    "        dataset.append((1, f'¿A qué precio está el juego {game}?'))\n",
    "        dataset.append((1, f'¿Cuál es el precio del juego {game}?'))\n",
    "        dataset.append((1, f'¿Cuál es el precio de {game}?'))\n",
    "        dataset.append((1, f'¿Cuánto cuesta {game}?'))\n",
    "        dataset.append((1, f'¿Cuánto cuesta el juego {game}?'))\n",
    "        dataset.append((1, f'¿Cuánto vale el juego {game}?'))\n",
    "        dataset.append((1, f'¿Cuánto vale {game}?'))\n",
    "        dataset.append((1, f'¿Cuál es el valor de {game}?'))\n",
    "        dataset.append((1, f'¿Cuál es el valor del juego {game}?'))\n",
    "\n",
    "        # Añadimos preguntas etiquetadas con '2'.\n",
    "        dataset.append((2, f'¿Quién es el desarrollador de {game}?'))\n",
    "        dataset.append((2, f'¿Quién desarrolló {game}?'))\n",
    "        dataset.append((2, f'¿Quién desarrolló el {game}?'))\n",
    "        dataset.append((2, f'¿Quién hizo el {game}?'))\n",
    "        \n",
    "        dataset.append((2, f'¿Quién publicó el {game}?'))\n",
    "        dataset.append((2, f'¿Quién distribuyó el {game}?'))\n",
    "        dataset.append((2, f'¿Quién es el distribuidor del juego {game}?'))\n",
    "        dataset.append((2, f'¿Quién es el distribuidor de {game}?'))\n",
    "\n",
    "        dataset.append((2, f'¿Qué genero tiene el juego {game}?'))\n",
    "        dataset.append((2, f'¿Qué género es {game}?'))\n",
    "        dataset.append((2, f'¿Qué genero es el juego {game}?'))\n",
    "        dataset.append((2, f'¿Cuáles generos tiene el juego {game}?'))\n",
    "        dataset.append((2, f'¿Cuáles son los géneros de {game}?'))\n",
    "    \n",
    "    print(f'Se hicieron un total de {len(dataset)} preguntas.')    \n",
    "    return dataset\n",
    "\n",
    "'''  Definimos una función para cargar el modelo de embedding BERT. '''\n",
    "def load_embedding_model():\n",
    "    model_name = 'dccuchile/bert-base-spanish-wwm-cased' # Definimos el nombre del modelo BERT en español\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name) # Cargamos el tokenizador de BERT\n",
    "    model = BertModel.from_pretrained(model_name) # Cargamos el modelo BERT\n",
    "    return tokenizer, model\n",
    "\n",
    "''' Definimos una función para dividir el texto en conjuntos de entrenamiento y prueba '''\n",
    "def text_split(dataset):\n",
    "    # Separamos los textos y las etiquetas del conjunto de datos\n",
    "    X = [text.lower() for label, text in dataset] \n",
    "    y = [label for label, text in dataset]\n",
    "    \n",
    "    # Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "''' Definimos una función para obtener embeddings de BERT para una lista de textos '''\n",
    "def get_bert_embeddings(texts, tokenizer, model, progress):\n",
    "    embeddings = [] # Inicializamos una lista para almacenar los embeddings\n",
    "    texts_seen = 0\n",
    "    \n",
    "    # Iteramos sobre cada texto en la lista\n",
    "    for text in texts:\n",
    "        texts_seen += 1\n",
    "        \n",
    "        # Preparamos el texto para el modelo BERT utilizando el tokenizador\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        \n",
    "        # Obtenemos los embeddings del texto sin calcular gradientes\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Añadimos el embedding del primer token a la lista de embeddings\n",
    "        embeddings.append(outputs.last_hidden_state[0][0].numpy())\n",
    "        \n",
    "        if progress: print(f'Progreso: {((texts_seen / len(texts)) * 100):.2f}%     ', end='\\r')\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b95886",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.6. *Funciones para Generar Respuesta del Chatbot*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ead90-7c85-4f62-bbfa-302a9ca5074a",
   "metadata": {},
   "source": [
    "En esta sección, me enfoco en seleccionar la información específica que se proporcionará como contexto al **Chatbot**, basándonos en la fuente de donde necesitemos extraer los datos de acuerdo con la pregunta formulada por el usuario. Dado que este proceso depende de predicciones, es posible que al **Chatbot** se le solicite información que podría considerar irrelevante para la consulta, en caso de acceder a una fuente que no dispone de los datos requeridos. Asimismo, se destaca la elección del modelo de lenguaje utilizado para generar las respuestas, el **Zephyr 7B β**. \n",
    "\n",
    "Después de probar varios modelos, este resultó ser mi preferido por la calidad de sus respuestas, siendo mi elección el resultado de un proceso de prueba y error, o más bien, de prueba y precisión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "044e2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Definimos una función para clasificar una consulta '''\n",
    "def query_classification(query_text, progress):\n",
    "    # Convertimos el texto de la consulta en una lista y lo pasamos a minúsculas\n",
    "    query_text = [query_text] \n",
    "    query_text_lower = [text.lower() for text in query_text]\n",
    "    \n",
    "    # Obtenemos los embeddings de BERT para el texto de la consulta\n",
    "    query_text_vectorized = get_bert_embeddings(query_text_lower, tokenizer, model, progress)\n",
    "    \n",
    "    # Utilizamos el modelo de regresión logística para predecir la intención de la consulta\n",
    "    prediction = modelo_LR.predict(query_text_vectorized)\n",
    "    return prediction[0]\n",
    "\n",
    "''' Definimos una función para extraer el nombre de un juego de una consulta '''\n",
    "def extract_name(search_string, games, similarity_threshold=80):\n",
    "    # Utilizamos fuzzywuzzy para encontrar la coincidencia más cercana\n",
    "    match, similarity = process.extractOne(search_string, [game for game in games])\n",
    "\n",
    "    # Verificamos si la similitud supera el umbral\n",
    "    if similarity >= similarity_threshold: return match\n",
    "    else: return 'No se encontro información al respecto'\n",
    "\n",
    "''' Definimos una función para buscar en un CSV '''\n",
    "def CSV_search(query, df):\n",
    "    name = extract_name(query, df['nombre'].tolist()) # Extraemos el nombre del juego de la consulta\n",
    "    \n",
    "    if name != 'No se encontro información al respecto':\n",
    "        result = df[df['nombre'] == name] # Buscamos el juego en el DataFrame por su nombre\n",
    "    \n",
    "        # Si encontramos el juego, devolvemos su precio\n",
    "        price = result.iloc[0]['precio']\n",
    "        return f'El precio de {name} es {str(price)} USD'\n",
    "    else: return 'No se encontro información al respecto'\n",
    "        \n",
    "''' Definimos una función para buscar en documentos PDF '''\n",
    "def PDF_search(query_text, embed, collection):\n",
    "    embedding_consulta = embed([query_text]).numpy().tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=embedding_consulta,\n",
    "        n_results=3  # Traemos los 3 resultados más cercanos\n",
    "    )\n",
    "    \n",
    "    # Recopilamos los textos de los documentos encontrados y los unimos\n",
    "    context_text = '\\n\\n---\\n\\n'.join([doc for doc in results[\"documents\"][0]])\n",
    "    return context_text\n",
    "\n",
    "''' Definimos una función para devolver la información, si se encuentra, correspondiente a la consulta en la BD de Grafos '''\n",
    "def GRAPH_search(query, G, similarity_threshold=80):\n",
    "    # Extraemos los nombres de juegos del grafo para la comparación\n",
    "    games = [n for n, d in G.nodes(data=True) if d.get('type') == 'game']\n",
    "    query_type = extract_query_type(query)  # Extraemos el tipo de consulta: 'developer', 'gender' o 'publisher'\n",
    "\n",
    "    # Extraemos el nombre del juego de la consulta\n",
    "    extracted_name = extract_name(query, games)\n",
    "\n",
    "    # Llamamos la función correspondiente dependiendo el tipo de consulta\n",
    "    if extracted_name:\n",
    "        if query_type == 'developer': \n",
    "            developer = query_developer(G, extracted_name)\n",
    "            query = f'El desarrollador de {extracted_name} es: {developer}'\n",
    "       \n",
    "        elif query_type == 'gender': \n",
    "            genders_list = query_gender(G, extracted_name)\n",
    "            genders = ''\n",
    "            for gender in genders_list: genders += f'{gender} '\n",
    "            query = f'Los géneros de {extracted_name} son: \\n{genders}' \n",
    "            query = query.replace('\\n', ' ', 1) # Reemplazamos el primer \"\\n' por un espacio\n",
    "            query = query.replace('\\n', ', ') # Reemplazamos los restantes '\\n' por comas\n",
    "        \n",
    "        elif query_type == 'publisher': \n",
    "            publisher = query_publisher(G, extracted_name)\n",
    "            query = f'El desarrollador de {extracted_name} es: {publisher}'\n",
    "            \n",
    "    else: query = 'No se encontró información relevante.'\n",
    "    return query\n",
    "\n",
    "''' Definimos una función para generar un prompt basado en la intención de la consulta '''\n",
    "def prompt_generate(query_text, intention, embed, collection, data, G):\n",
    "    # Dependiendo de la intención, buscamos en los documentos PDF o en el CSV\n",
    "    if intention == 0: context_text = PDF_search(query_text, embed, collection)\n",
    "    elif intention == 1: context_text = CSV_search(query_text, data)\n",
    "    elif intention == 2: context_text = GRAPH_search(query_text, G)\n",
    "\n",
    "    # Formateamos el prompt utilizando la plantilla y los datos obtenidos\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    return prompt\n",
    "\n",
    "''' Definimos una función para generar una respuesta usando la API del modelo LLM '''\n",
    "def generate_answer(prompt: str, max_new_tokens: int = 768) -> None:\n",
    "    try:\n",
    "        # Configuramos la API con la clave y la URL\n",
    "        api_key = API_KEY  # Ingresamos nuestro API KEY de HugginFace\n",
    "        api_url = 'https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta'\n",
    "        headers = {'Authorization': f'Bearer {api_key}'}\n",
    "        \n",
    "        # Preparamos los datos para enviar a la API\n",
    "        data = {\n",
    "            'inputs': prompt,\n",
    "            'parameters': {\n",
    "                'max_new_tokens': max_new_tokens,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.95\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Enviamos una solicitud POST a la API y obtenemos la respuesta\n",
    "        response = requests.post(api_url, headers=headers, json=data)\n",
    "        generated_text = response.json()[0]['generated_text']\n",
    "        \n",
    "        # Extraemos la respuesta del texto generado y la imprimimos\n",
    "        split_text = generated_text.split('Respuesta:') \n",
    "        answer = split_text[1].strip().split('\\n')[0]\n",
    "\n",
    "        return print(f'Respuesta: {answer}')\n",
    "    except Exception as e: print(f'Un error a ocurrido: {e}') # En caso de un error, imprimimos el mensaje de error\n",
    "\n",
    "''' Definimos una función principal para el chatbot '''\n",
    "def chatbot(embed, colecction, data, G):\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        query = input('Ingrese su consulta: ')\n",
    "    \n",
    "        # Verificar si la consulta no está vacía\n",
    "        if query:\n",
    "            # Asegurar que la consulta comienza con \"¿\" y termina con \"?\"\n",
    "            if not query.startswith('¿'): query = '¿' + query\n",
    "            if not query.endswith('?'): query = query + '?'\n",
    "            break\n",
    "        else: print('Ingrese una consulta válida.')\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    print(f'Consulta procesada: {query}\\n')\n",
    "    \n",
    "    prediction = query_classification(query, False) # Clasificamos la consulta para determinar su intención\n",
    "    prompt = prompt_generate(query, prediction, embed, colecction, data, G) # Generamos un prompt basado en la consulta y su intención\n",
    "    answer = generate_answer(prompt) # Generamos una respuesta basada en el prompt y la devolvemos\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacb63d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. **Creación de Fuentes de Información**\n",
    "\n",
    "Desde esta sección hasta abajo solo se ejecutan las funciones que se crearon con anterioridad junto a una desmotración paso a paso sobre que hace el **ChatBot**. Al final también hay una DEMO por si se quiere consultar directamente al **Chatbot** con varias consultas consecutivas para ir evaluando su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffe79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traemos los nombres de los juegos\n",
    "games = bring_game_names()\n",
    "\n",
    "# Creamos los PDFs y CSV con la información necesaria para el chatbot\n",
    "game_details(games[:10000]) # Lo limitamos a 10.000 porque sino tardaría muchisimo tiempo en traer todos (+180.000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11dbf5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. **Carga y Limpieza de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d597a",
   "metadata": {},
   "source": [
    "## 6.1. *Documentos PDF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e54f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los documentos PDF\n",
    "raw_documents = load_pdf_documents(PDF_PATH)\n",
    "\n",
    "# Limpiamos los documentos\n",
    "documents = clean_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7cec1a",
   "metadata": {},
   "source": [
    "## 6.2. *Datos Tabulares (CSV)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36538f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y limpiamos el csv\n",
    "data = load_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ee546",
   "metadata": {},
   "source": [
    "## 6.3 *Recolección y Etiquetado de Preguntas para Clasificación de Intenciones*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294ce9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se hicieron un total de 26490 preguntas.\n"
     ]
    }
   ],
   "source": [
    "# Creamos las preguntas y sus etiquetas\n",
    "dataset_intentions = collection_labeling(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e1ed8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7. **Procesamiento de Datos para Base de Datos Chroma (PDFs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f402fb",
   "metadata": {},
   "source": [
    "## 7.1. *División de los Documentos en Pequeñas Partes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8779723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se dividió 2 documentos en 2649 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Dividimos los documentos en pequeñas partes\n",
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581270a",
   "metadata": {},
   "source": [
    "## 7.2. *Conversión a Embeddings y Almacenamiento en Chroma*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "328fdb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se guardaron 2649 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Convertimos las partes utilizando 'Embeddings' y las almacenamos en una base de datos'Chroma'\n",
    "embed, collection = save_to_chroma(chunks, model_embedding_chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48bed1-85bc-41db-8a7b-ad85a3000b65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8. **Creación de la Base de Datos de Grafos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b2e6688-7e41-4686-8030-ef24800cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la Base de Datos de Grafos\n",
    "G = create_graph_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c55ced",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 9. **Entrenamiento del Clasificador de Intenciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbca933",
   "metadata": {},
   "source": [
    "## 9.1. *Cargamos el Modelo de Embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9adca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el modelo\n",
    "tokenizer, model = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e3efa",
   "metadata": {},
   "source": [
    "## 9.2. *División en Conjunto de Entrenamiento y Prueba y Tokenización*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69013f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso: 100.00%     \r"
     ]
    }
   ],
   "source": [
    "# Dividimos nuestro Dataset\n",
    "X_train, X_test, y_train, y_test = text_split(dataset_intentions)\n",
    "\n",
    "# Obtenemos los embeddings de BERT para los conjuntos de entrenamiento y prueba\n",
    "X_train_vectorized = get_bert_embeddings(X_train, tokenizer, model, True)\n",
    "X_test_vectorized = get_bert_embeddings(X_test, tokenizer, model, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc890f",
   "metadata": {},
   "source": [
    "## 9.3. *Entrenamiento y Evaluación del Modelo Clasificador*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7df08e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión Regresión Logística: 0.9998112495281238\n",
      "Reporte de clasificación Regresión Logística:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1402\n",
      "           1       1.00      1.00      1.00      1622\n",
      "           2       1.00      1.00      1.00      2274\n",
      "\n",
      "    accuracy                           1.00      5298\n",
      "   macro avg       1.00      1.00      1.00      5298\n",
      "weighted avg       1.00      1.00      1.00      5298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos y entrenamamos el modelo de Regresión Logística Multinomial\n",
    "modelo_LR = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "modelo_LR.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluamos el modelo de Regresión Logística\n",
    "y_pred_LR = modelo_LR.predict(X_test_vectorized)\n",
    "acc_LR = accuracy_score(y_test, y_pred_LR)\n",
    "report_LR = classification_report(y_test, y_pred_LR, zero_division=1)\n",
    "\n",
    "print('Precisión Regresión Logística:', acc_LR)\n",
    "print('Reporte de clasificación Regresión Logística:\\n', report_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f7dbd",
   "metadata": {},
   "source": [
    "## 9.4. *Validación del Modelo Clasificador*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6952446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: \"¿Cuanto vale Traffix?\"\n",
      "Clasificación predicha: Consulta CSV\n",
      "\n",
      "Texto: \"¿De qué se trata Mystery Island - Hidden Object Games?\"\n",
      "Clasificación predicha: Consulta PDF\n",
      "\n",
      "Texto: \"¿Cuáles son los requisitos de Nyanco?\"\n",
      "Clasificación predicha: Consulta PDF\n",
      "\n",
      "Texto: \"¿Quién desarollo de Pushy and Pully in Blockland?\"\n",
      "Clasificación predicha: Consulta Grafos\n",
      "\n",
      "Texto: \"¿Cuanto valdria DOTORI?\"\n",
      "Clasificación predicha: Consulta CSV\n",
      "\n",
      "Texto: \"¿Quién desarollo Escape Dead Earth?\"\n",
      "Clasificación predicha: Consulta Grafos\n",
      "\n",
      "Texto: \"¿Cuáles son los géneros de DinosaurIsland?\"\n",
      "Clasificación predicha: Consulta Grafos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nuevas frases para clasificar como validación del modelo\n",
    "new_phrases = [\n",
    "    '¿Cuanto vale Traffix?',\n",
    "    '¿De qué se trata Mystery Island - Hidden Object Games?',\n",
    "    '¿Cuáles son los requisitos de Nyanco?',\n",
    "    '¿Quién desarollo de Pushy and Pully in Blockland?',\n",
    "    '¿Cuanto valdria DOTORI?',\n",
    "    '¿Quién desarollo Escape Dead Earth?',\n",
    "    '¿Cuáles son los géneros de DinosaurIsland?'\n",
    "]\n",
    "\n",
    "# Preprocesamiento y vectorización de las nuevas frases\n",
    "new_phrases_lower = [text.lower() for text in new_phrases]\n",
    "new_phrases_vectorized = get_bert_embeddings(new_phrases_lower, tokenizer, model, False)\n",
    "\n",
    "# Hacemos predicciones con el modelo entrenado\n",
    "new_predictions = modelo_LR.predict(new_phrases_vectorized)\n",
    "\n",
    "# Mostramos las predicciones junto con las frases\n",
    "for text, label in zip(new_phrases, new_predictions):\n",
    "    print(f'Texto: \"{text}\"')\n",
    "    print(f'Clasificación predicha: {labels[label][1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd1230",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 10. **Integración y Flujo de Trabajo del Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9bf36",
   "metadata": {},
   "source": [
    "## 10.1. Definimos la consulta a hacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "414346f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pregunta hecha por el 'usuario'\n",
    "query = \"¿Cuál es la descripción de Game Developer Simulator?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e323bd",
   "metadata": {},
   "source": [
    "## 10.2. *Clasificación de Pregunta de Usuario*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62788e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificamos la query para saber que base de datos usar\n",
    "prediction = query_classification(query, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb32801",
   "metadata": {},
   "source": [
    "## 10.3. *Búsqueda en la Base de Datos Correspondiente y Generación del Prompt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb46c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos el contexto correspondiente y generamos el prompt\n",
    "prompt = prompt_generate(query, prediction, embed, collection, data, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb38559",
   "metadata": {},
   "source": [
    "## 10.4. *Generación de Respuesta con el Chatbot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f15da004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: En el Simulador de un desarrollador de juegos, se puede comprender todo lo subtil de esta profesión, comprendar cómo funciona la industria de los juegos y todas las dificultades de un solo desarrollador.\n"
     ]
    }
   ],
   "source": [
    "# Generamos la respuesta del Chatbot\n",
    "answer = generate_answer(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6279d53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 11. **Demo del Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc51e38d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta procesada: ¿Cuál es la descripción de Developer Simulator?\n",
      "\n",
      "Respuesta: La descripción corta de Developer Simulator es: \"En el Simulador de un desarrollador de juegos, podrá comprender todas las sutilezas de esta profesión, comprender cómo funciona la industria de los juegos y todas las dificultades de un solo desarrollador.\"\n"
     ]
    }
   ],
   "source": [
    "chatbot(embed, collection, data, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadc53f",
   "metadata": {},
   "source": [
    "# 12. **Informe sobre Agentes Inteligentes (LLM libres)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81a468",
   "metadata": {},
   "source": [
    "## 12.1. *Estado del Arte de las Aplicaciones Actuales*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7a798",
   "metadata": {},
   "source": [
    "La evolución de los agentes inteligentes y los modelos LLM ha revolucionado diversas áreas, desde el análisis de datos hasta la interacción en lenguaje natural. Plataformas como *OpenAgents* buscan democratizar el acceso a estas tecnologías, ofreciendo una base para la creación y el uso de agentes inteligentes basados en LLM de manera más abierta y accesible. \n",
    "\n",
    "Estos agentes pueden especializarse en tareas como el análisis de datos, la integración con **APIs** para tareas cotidianas y la navegación web anónima. La interacción con estos agentes se facilita a través de interfaces web, permitiendo a los usuarios, desarrolladores e investigadores colaborar y expandir sus funcionalidades.\n",
    "\n",
    "Además, el impacto de los modelos **LLM** se ha notado en la mejora de la interacción usuario-máquina, permitiendo a cualquier persona, sin necesidad de conocimientos técnicos, interactuar satisfactoriamente con estos sistemas. Los agentes inteligentes se han vuelto parte de la vida cotidiana, ofreciendo desde asistencia virtual hasta sugerencias de productos personalizadas, lo que demuestra su capacidad de aprendizaje y adaptación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9373946",
   "metadata": {},
   "source": [
    "## 12.2. *Problemática a Solucionar con un Sistema Multiagente*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb78a3",
   "metadata": {},
   "source": [
    "Una problemática relevante en la sociedad actual es la gestión eficiente de las respuestas ante emergencias, como desastres naturales o accidentes urbanos. La coordinación rápida y efectiva entre los diferentes servicios de emergencia puede ser la diferencia entre salvar vidas y mitigar daños.\n",
    "\n",
    "El sistema multiagente propuesto incluiría los siguientes agentes, cada uno especializado en una tarea crítica de la gestión de emergencias:\n",
    "* **Agente de Detección**: Utilizaría tecnologías de procesamiento de lenguaje natural y análisis de datos para monitorear redes sociales, noticias y sensores IoT (sensores inteligentes) para detectar posibles emergencias en tiempo real.\n",
    "\n",
    "* **Agente de Coordinación**: Este agente, basado en modelos de toma de decisiones y optimización, sería el encargado de evaluar la información recopilada y coordinar la respuesta entre los diferentes servicios de emergencia\n",
    "\n",
    "* **Agente de Logística**: Especializado en la optimización de rutas y recursos, este agente garantizaría la asignación eficiente de recursos y personal de emergencia al lugar del incidente.\n",
    "\n",
    "* **Agente de Comunicación**: Utilizaría LLM para interactuar con el público y los servicios de emergencia, proporcionando información actualizada, recomendaciones y recopilando datos relevantes del terreno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75625af4",
   "metadata": {},
   "source": [
    "## 12.3. *Ejemplo de Conversación*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67de255",
   "metadata": {},
   "source": [
    "**Agente de Detección**: *\"He detectado un aumento significativo de mensajes en redes sociales sobre un posible incendio forestal en la región de la Sierra Norte. Los sensores de temperatura en la zona también muestran lecturas anómalas.\"*\n",
    "\n",
    "**Agente de Coordinación**: *\"Recibido, Agente de Detección. Solicitando al Agente de Logística que evalúe la disponibilidad y la ubicación de los equipos de bomberos más cercanos y la mejor ruta para llegar al lugar del incendio.\"*\n",
    "\n",
    "**Agente de Logística**: *\"El equipo de bomberos más cercano está en la estación de San Miguel, a 10 km del incendio. La ruta más rápida está actualmente despejada y tomará aproximadamente 15 minutos. Estoy coordinando la movilización inmediata.\"*\n",
    "\n",
    "**Agente de Comunicación**: *\"Informando a la población local sobre el incendio. Mensaje enviado: 'Alerta de incendio en la Sierra Norte. Por favor, eviten el área y sigan las instrucciones de evacuación si se encuentran cerca. Manténganse informados para más actualizaciones.' También estoy informando a los equipos de bomberos con detalles sobre la situación y las coordenadas exactas del incendio.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a69445",
   "metadata": {},
   "source": [
    "## 12.4. *Fuentes de Información*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554efb0",
   "metadata": {},
   "source": [
    "[**Unite.AI**: *OpenAgents: una plataforma abierta para agentes lingüísticos en la naturaleza*](https://www.unite.ai/es/openagents-una-plataforma-abierta-para-agentes-lingüísticos-en-la-naturaleza/)\n",
    "\n",
    "[**Hiberus**: *Estado del arte de las IAs: la inteligencia artificial en la actualidad*](https://www.hiberus.com/crecemos-contigo/sobre-la-inteligencia-artificial-en-la-actualidad/)\n",
    "\n",
    "[**UNIR**: *Los agentes inteligentes: Funciones y ejemplos de uso*](https://www.unir.net/ingenieria/revista/agentes-inteligentes/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
